{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/andreastsimerikas/pneumonia-classification?scriptVersionId=95314242\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\n\ntraindf = pd.read_csv('../input/pneumoniasized/labels_train.csv', dtype=str)\n# path for the square images with black padding \ntrain_path = '../input/pneumoniasized/sized_train'\n\n# Class_0 = Normal, Class_1 = Bacterial-Pneumonia, Class_2 = Viral-Pneumonia\n# Splitting the main dataframe into smaller class spesific dataframes \nclass_0=traindf.loc[traindf.class_id == '0']\nclass_0.reset_index(drop=True,inplace=True)\nclass_1=traindf.loc[traindf.class_id == '1']\nclass_1.reset_index(drop=True,inplace=True)\nclass_2=traindf.loc[traindf.class_id == '2']\nclass_2.reset_index(drop=True,inplace=True)\n\n# Combining a portion of our class dataframes into one dataframe for training, maintaining the distribution \ntrain = class_0[:920]\ntrain = train.append(class_1[:1678])\ntrain = train.append(class_2[:905])\ntrain.reset_index(drop=True,inplace=True)\n\n# Combining the rest of our class dataframes into one dataframe for testing\ntest = class_0[920:]\ntest = test.append(class_1[1678:])\ntest = test.append(class_2[905:])\ntest.reset_index(drop=True,inplace=True)\n\n# Define the number of splits for cross-validation\nsplit_num = 3\nkfold = StratifiedKFold(n_splits=split_num, shuffle=True)\n\nVALIDATION_ACCURACY = []\nVALIDAITON_LOSS = []","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-16T12:33:04.94233Z","iopub.execute_input":"2022-02-16T12:33:04.942654Z","iopub.status.idle":"2022-02-16T12:33:06.021989Z","shell.execute_reply.started":"2022-02-16T12:33:04.942575Z","shell.execute_reply":"2022-02-16T12:33:06.021108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\n# Visualizing the distribution of our dataset\nplt.hist(train['class_id'],rwidth=0.8, bins=3, histtype='bar', label='Classes Distribution')","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:16:40.246307Z","iopub.execute_input":"2022-02-15T19:16:40.246879Z","iopub.status.idle":"2022-02-15T19:16:40.420801Z","shell.execute_reply.started":"2022-02-15T19:16:40.24684Z","shell.execute_reply":"2022-02-15T19:16:40.420013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport cv2\nimport matplotlib.pyplot as plt\n\npath='../input/pneumoniasized/sized_train/'\n\nfig = plt.figure(figsize=(20, 20))\n  \n# setting values to rows and column variables\nrows = 1\ncolumns = 3\n\n# image indexes to select\nindex_1=0\nindex_2=1700\nindex_3=3200\n  \n# reading images\nImage1 = cv2.imread(path+train.iloc[index_1][0],1)\nImage1_label = train.iloc[index_1][1]\nImage2 = cv2.imread(path+train.iloc[index_2][0],1)\nImage2_label = train.iloc[index_2][1]\nImage3 = cv2.imread(path+train.iloc[index_3][0],1)\nImage3_label = train.iloc[index_3][1]\n  \n# Adds a subplot at the 1st position\nfig.add_subplot(rows, columns, 1)\n  \n# showing normal case image \nplt.imshow(Image1)\nplt.axis('off')\nplt.title(Image1_label)\n  \n# Adds a subplot at the 2nd position\nfig.add_subplot(rows, columns, 2)\n  \n# showing bacterial pneumonia image\nplt.imshow(Image2)\nplt.axis('off')\nplt.title(Image2_label)\n  \n# Adds a subplot at the 3rd position\nfig.add_subplot(rows, columns, 3)\n  \n# showing viral pneumonia image\nplt.imshow(Image3)\nplt.axis('off')\nplt.title(Image3_label)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:16:42.031785Z","iopub.execute_input":"2022-02-15T19:16:42.032335Z","iopub.status.idle":"2022-02-15T19:16:43.334488Z","shell.execute_reply.started":"2022-02-15T19:16:42.032295Z","shell.execute_reply":"2022-02-15T19:16:43.333729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input, average, BatchNormalization\nfrom tensorflow.keras.applications import EfficientNetB3\nfrom tensorflow.keras.initializers import *\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\nimg_size = 300\n\n# Creates and returns an EfficientNetB3 model with different classification layers\ndef create_model(input_shape, n_classes):\n    \n    # Input and preprocess needed for EfficientNetB3\n    inputs = Input(input_shape)\n    prep = preprocess_input(inputs)\n\n    # EfficientNetB3 without top classification layers\n    conv_base = EfficientNetB3(include_top=False,\n                            weights='imagenet',\n                            input_tensor=prep)\n    \n    # Frezzing all BatchNormalization layers\n    for layer in conv_base.layers:\n        if isinstance(layer, BatchNormalization):\n            layer.trainable=False\n    # Frezzing the first two blocks\n    for layer in conv_base.layers[:73]:       \n        layer.trainable=False\n\n    # Top classification layers        \n    top_model = GlobalAveragePooling2D()(conv_base.output)\n    top_model = Dense(512, activation='relu', kernel_initializer=\"glorot_normal\")(top_model)\n    top_model = Dropout(0.3)(top_model)\n    top_model = Dense(128, activation='relu', kernel_initializer=\"glorot_normal\")(top_model)\n    output = Dense(n_classes, activation='softmax')(top_model)\n    \n    model = Model(inputs, output)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-16T12:35:35.08645Z","iopub.execute_input":"2022-02-16T12:35:35.08686Z","iopub.status.idle":"2022-02-16T12:35:35.098227Z","shell.execute_reply.started":"2022-02-16T12:35:35.086827Z","shell.execute_reply":"2022-02-16T12:35:35.096776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input, average\nfrom tensorflow.keras.applications import DenseNet201\nfrom tensorflow.keras.initializers import *\nfrom tensorflow.keras.applications.densenet import preprocess_input\n\nimg_size = 224\n\n# Creates and returns a DenseNet121 model with different top classification layers\ndef create_model(input_shape, n_classes):\n    \n    # Input and preprocess needed for DenseNet201\n    inputs = Input(input_shape)\n    prep = preprocess_input(inputs)\n\n    # DenseNet201 without top classification layers and trainable weights\n    conv_base = DenseNet201(include_top=False,\n                            weights='imagenet',\n                            input_tensor=prep)\n\n    # Top classification layers      \n    top_model = GlobalAveragePooling2D()(conv_base.output)\n    top_model = Dense(512, activation='relu', kernel_initializer=\"glorot_normal\")(top_model)\n    top_model = Dropout(0.3)(top_model)\n    top_model = Dense(128, activation='relu', kernel_initializer=\"glorot_normal\")(top_model)\n    output = Dense(n_classes, activation='softmax')(top_model)\n    \n    model = Model(inputs, output)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:31:41.814951Z","iopub.execute_input":"2022-02-15T20:31:41.815202Z","iopub.status.idle":"2022-02-15T20:31:46.681053Z","shell.execute_reply.started":"2022-02-15T20:31:41.815174Z","shell.execute_reply":"2022-02-15T20:31:46.680347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\n# Dummy model for visualization \nmodel = create_model((100,100,3), 3)\nplot_model(model)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T17:20:09.706238Z","iopub.execute_input":"2022-02-15T17:20:09.706802Z","iopub.status.idle":"2022-02-15T17:20:19.721275Z","shell.execute_reply.started":"2022-02-15T17:20:09.706758Z","shell.execute_reply":"2022-02-15T17:20:19.720457Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Different ImageDataGenerators for train and validation batches without live augmentaion\ntdatagen = ImageDataGenerator()\nvdatagen = ImageDataGenerator()  ","metadata":{"execution":{"iopub.status.busy":"2022-02-16T12:35:43.284413Z","iopub.execute_input":"2022-02-16T12:35:43.284729Z","iopub.status.idle":"2022-02-16T12:35:43.290468Z","shell.execute_reply.started":"2022-02-16T12:35:43.284699Z","shell.execute_reply":"2022-02-16T12:35:43.28914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\nfrom tensorflow.keras.backend import clear_session\nfrom matplotlib import pyplot as plt\n\nsave_dir = './'\nfold_no = 1\nbatch_size = 10\nn_classes = 3\nn_epochs = 7\nfold_no = 1\ninput_shape=(img_size,img_size,3)\n\n\n# Splitting image names and labels for kfold cross-validation\nimgs = train[['file_name']]\nlabels = train[['class_id']]\n\n# Returns current fold's model\ndef get_model_name(num):\n    return 'model_'+str(num)+'.h5'\n\n# Training with stratified k-fold cross-validation\nfor train_index, val_index in kfold.split(imgs,labels):\n\n    # Creating current fold dataframes for training and validation\n    fold_train_df = train.iloc[train_index]\n    fold_valid_df = train.iloc[val_index]\n    \n    # Creating train and validation batches using the ImageDataGenerators and the dataframes we created previously\n    train_batches = tdatagen.flow_from_dataframe(fold_train_df, \n                                                directory=train_path,\n                                                x_col=\"file_name\", \n                                                y_col=\"class_id\",\n                                                batch_size=batch_size, \n                                                class_mode=\"categorical\", \n                                                target_size=(img_size,img_size))\n\n    valid_batches = vdatagen.flow_from_dataframe(fold_valid_df, \n                                                directory=train_path,\n                                                x_col=\"file_name\", \n                                                y_col=\"class_id\",\n                                                batch_size=batch_size, \n                                                class_mode=\"categorical\", \n                                                target_size=(img_size,img_size))\n\n    # Creating the model with our top classification layers  \n    model = create_model(input_shape, n_classes)\n    model._name=\"model_\"+str(fold_no)\n\n    # Compiling the model with chosen optimizer, loss and metrics\n    model.compile(optimizer=Adam(learning_rate=1e-4), \n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    \n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {fold_no} ...')\n    \n    #-----------------------callbacks----------------------------\n    from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n\n    # Returns leraning rate based on epoch\n    def scheduler(epoch, lr):\n        if epoch > 2:\n            lr=1e-5\n            print(lr)\n            return lr\n        else:\n            return lr\n        \n    lr_scheduler = LearningRateScheduler(scheduler)\n\n    # Saves best model of every fold based on maximum validation accuracy\n    model_checkpoint= ModelCheckpoint(filepath=\"model_\"+str(fold_no)+\".h5\",\n                                      monitor='val_accuracy',\n                                      mode='max',\n                                      save_best_only=True,\n                                      verbose=1)\n    # List of all our callbacks\n    callbacks_list= [model_checkpoint,lr_scheduler]\n    #-----------------------callbacks----------------------------\n    \n    # Creates balanced class weights \n    weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_batches.classes), y=train_batches.classes)\n    cw = dict(zip(np.unique(train_batches.classes), weights))\n    print('Class weights: ' + str(cw))\n    \n    # Training using train and validation batches, our callbacks and class weights\n    history = model.fit(train_batches,\n                        epochs=n_epochs,\n                        validation_data=valid_batches,\n                        callbacks=callbacks_list,\n                        class_weight=cw,\n                        verbose=1)  \n    \n    # Plot of current's fold accuracy, loss, validation accuracy and validation loss\n    %matplotlib inline\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('fold '+str(fold_no))\n    plt.ylabel('accuracy/loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_acc', 'val_acc', 'train_loss', 'val_loss'], loc='best')\n    plt.grid(visible=True)\n    plt.show()  \n\n    model.load_weights(\"./model_\"+str(fold_no)+\".h5\")\n    \n    # Fold's saved model evaluation on current's fold validation set\n    results = model.evaluate(valid_batches) \n    results = dict(zip(model.metrics_names,results))\n\n    # Appending the results of the fold \n    VALIDATION_ACCURACY.append(results['accuracy'])\n    VALIDAITON_LOSS.append(results['loss'])  \n\n    clear_session()\n    fold_no += 1 ","metadata":{"execution":{"iopub.status.busy":"2022-02-16T12:35:45.718492Z","iopub.execute_input":"2022-02-16T12:35:45.71877Z","iopub.status.idle":"2022-02-16T13:03:40.918742Z","shell.execute_reply.started":"2022-02-16T12:35:45.718741Z","shell.execute_reply":"2022-02-16T13:03:40.917694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statistics import mean\nfrom tensorflow.keras.models import load_model\n\nnets = split_num\nmodels = [0] *nets\n\n# List with all the fold models\nfor j in range(nets):\n    models[j] = load_model(\"./model_\"+str(j+1)+\".h5\")\n    \n# Mean of all the folds\nprint(mean(VALIDATION_ACCURACY))\nprint(mean(VALIDAITON_LOSS))    ","metadata":{"execution":{"iopub.status.busy":"2022-02-16T13:03:40.924383Z","iopub.execute_input":"2022-02-16T13:03:40.924617Z","iopub.status.idle":"2022-02-16T13:03:53.263768Z","shell.execute_reply.started":"2022-02-16T13:03:40.924587Z","shell.execute_reply":"2022-02-16T13:03:53.262741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creates and returns the average ensemble model of all the folds\ndef ensemble(models, model_input):\n    \n    Models_output=[model(model_input) for model in models]\n    Avg = average(Models_output)\n    \n    modelEnsemble = Model(inputs=model_input, outputs=Avg, name='ensemble')\n    modelEnsemble.compile(Adam(learning_rate=1e-5), \n                          loss='categorical_crossentropy', \n                          metrics=['accuracy'])\n    modelEnsemble.summary()\n    \n    return modelEnsemble\n\n# Input layer for the ensemble\nmodel_input = Input(shape=models[0].input_shape[1:])\n# Creates the ensemble model\nensemble_model = ensemble(models, model_input)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T13:03:53.265319Z","iopub.execute_input":"2022-02-16T13:03:53.265548Z","iopub.status.idle":"2022-02-16T13:03:57.745828Z","shell.execute_reply.started":"2022-02-16T13:03:53.265514Z","shell.execute_reply":"2022-02-16T13:03:57.74464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom tensorflow.keras.models import load_model\n\n# ImageDataGenerator without augmentaions for testing\ntest_datagen = ImageDataGenerator()\n\n# Test batches with test ImageDataGenerator using our test dataframe\ntest_batches = test_datagen.flow_from_dataframe(\n    dataframe=test,\n    directory=train_path,\n    x_col='file_name',\n    y_col='class_id',\n    batch_size=10,\n    class_mode='categorical',\n    shuffle=False,\n    target_size=(img_size,img_size)\n)\n\n# loading and evaluating all our fold models on the test data\nfor j in range(nets):\n    models[j] = load_model(\"./model_\"+str(j+1)+\".h5\")\n    print(\"model_\"+str(j+1))\n    models[j].evaluate(test_batches)\n\n# evaluating the average ensemble model on the test data    \nprint(\"average ensemble\")\nresults = ensemble_model.evaluate(test_batches)\npredictions = ensemble_model.predict(test_batches,verbose=1)\n\n# plotting a confusion matrix from the ensemble model predictions\ncm = confusion_matrix(test_batches.classes, np.argmax(predictions, axis=-1))\nlabels = ['0','1','2']\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T13:03:57.748409Z","iopub.execute_input":"2022-02-16T13:03:57.748711Z","iopub.status.idle":"2022-02-16T13:05:43.727019Z","shell.execute_reply.started":"2022-02-16T13:03:57.748666Z","shell.execute_reply":"2022-02-16T13:05:43.725953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nimport scikitplot as skplt\n\n# ensemble model classification report with precision, recall, f1-score and accuracy\ny = np.argmax(predictions, axis=-1)\ncr = classification_report(y_true=test_batches.classes, y_pred=y, target_names=test_batches.class_indices)\nprint(cr)\n\n# roc curves of all the classes\nskplt.metrics.plot_roc(test_batches.classes, predictions)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T13:07:10.145204Z","iopub.execute_input":"2022-02-16T13:07:10.145499Z","iopub.status.idle":"2022-02-16T13:07:10.512052Z","shell.execute_reply.started":"2022-02-16T13:07:10.145469Z","shell.execute_reply":"2022-02-16T13:07:10.511006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom tensorflow.keras.models import load_model\n\ntest_datagen = ImageDataGenerator()\ntest_path = '../input/pneumonia/sized/sized_test_df'\ntest_images = []\n\ni=1\nfor img in os.listdir(test_path):\n    test_images.append(img)\n    i+=1\n   \nsubmision = pd.DataFrame(test_images,columns=['file_name'])\n\ntest_batches = test_datagen.flow_from_dataframe(\n    dataframe=submision,\n    directory=test_path,\n    x_col=\"file_name\",\n    batch_size=10,\n    class_mode=None,\n    shuffle=False,\n    target_size=(img_size,img_size)\n)\n\npredictions = ensemble_model.predict(test_batches,verbose=1)\n\npreds = np.argmax(predictions, axis=-1) \nprint(preds)\n\nsubmision['class_id'] = preds\nsubmision.to_csv('submision39.csv',index=False)","metadata":{},"execution_count":null,"outputs":[]}]}